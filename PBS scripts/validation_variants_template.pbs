#!/bin/bash

##########################
#                        #
#   The PBS directives   #
#                        #
##########################

# Define the shell in which your jobs should run. Shouldn't really be changed
# unless you have a very specific reason for doing so
#PBS -S /bin/bash


# Define the name for the job
#PBS -N mart_export_increment



# Defining the wall time for the job
#PBS -l walltime=40:00:00



#PBS -o "/group/tran3/duytran/ngocduy.tran/Python scripts/"
#PBS -e /group/tran3/duytran/ngocduy.tran/logs/mart_export_increment_human_encode_snv.err





# Selecting which queue to send the job to
#PBS -q batch





# Defining the amount of memory you require
#PBS -l mem=150GB




# Define the email address to be used in correspondence
#PBS -M ngocduy.tran@mcri.edu.au


# Define the number of nodes and cores you require
#PBS -l nodes=1:ppn=10



# Used to define which project job is associated with
#PBS -A tran3


### END OF PBS OPTIONS



##########################################
#                                        #
#   Output some useful job information.  #
#                                        #
##########################################

#echo ------------------------------------------------------
#echo -n 'Job is running on node '; cat $PBS_NODEFILE
#echo ------------------------------------------------------
#echo PBS: qsub was run on $PBS_O_HOST
#echo PBS: originating queue is $PBS_O_QUEUE
#echo PBS: executing queue is $PBS_QUEUE
#echo PBS: working directory is $PBS_O_WORKDIR
#echo PBS: execution mode is $PBS_ENVIRONMENT
#echo PBS: job identifier is $PBS_JOBID
#echo PBS: job name is $PBS_JOBNAME
#echo PBS: node file is $PBS_NODEFILE
#echo PBS: current home directory is $PBS_O_HOME
#echo PBS: temporary directory on node is $TMPDIR
#echo PBS: PATH = $PBS_O_PATH
#echo ------------------------------------------------------


#>>>>>>>>>> Steps to excecute your job goes here <<<<<<<<<<<<<<<<<<<<<

# 2 things to modify: pbs file and -p only. Also make folder for chromosome in validation merging test folder

# Change to the temporary folder cretated for this job on the comp node
# environment setup and change to desired folder for manipulating
cd /group/tran3/duytran/ngocduy.tran/Python\ scripts/bedtools_merging/Validation
source /home/ngocduy.tran/miniconda3/etc/profile.d/conda.sh

conda activate base
conda activate /home/ngocduy.tran/tran3/duytran/win/
module load bedtools 



cd ~
cd /group/tran3/duytran/ngocduy.tran/Python\ scripts/bedtools_merging

# output of this code is output_even_snv.txt and output_odd.txt; firstly init 
# with mart_export_increment.txt
python validation_variants_merging_categorical.py -c mart_export_increment > logging/mart_export_increment_print_logging.txt

    # continuous variables
        # copy the validation file: mart_export_increment.txt (simple version) to continuous_merging 
        # problem here unable to cp
cp Validation/mart_export_increment.txt ../continuous_merging
        # bedtools intersect with ncer scores
cd ~
cd /group/tran3/duytran/ngocduy.tran/Python\ scripts/continuous_merging
# whenever -loj will require to drop some duplicates of the ncer scores file not the mcri_merging_categorical file
bedtools intersect -loj -a mart_export_increment.txt -b ncer_transformed_all_chr.bed > mart_export_increment_ncer.txt
cd ..
# create the dropped_duplicates version, use later in pasting
python drop_dups_continuous.py -c mart_export_increment > logging/mart_export_increment_ncer_dups.txt
cd continuous_merging
# should add drop here
        # bedtools intersect with dickel scores
bedtools intersect -loj -a mart_export_increment.txt -b unfiltered_hg38.bed > mart_export_increment.txt
cd ..
# drop dups
python drop_dups_continuous_dickel.py -c mart_export_increment > logging/mart_export_increment_dickel_dups.txt

    # merge continuous with categorical
        # cut the columns from dickel and ncer to merge with the merged categorical and continuous version, which is output_even_snv.txt
cd ~
cd /group/tran3/duytran/ngocduy.tran/Python\ scripts/bedtools_merging/Validation\ merging\ test
# # problem here
cp mart_export_increment/output_even_snv.txt ../../continuous_merging/
cd ../../continuous_merging
mv output_even_snv.txt mart_export_increment_categorical.txt
paste -d"\t" mart_export_increment_categorical.txt <(cut -f7 mart_export_increment_ncer_drop_dups.txt) > mart_export_increment_cat_ncer.txt
paste -d"\t" mart_export_increment_cat_ncer.txt <(cut -f7 mart_export_increment_drop_dups.txt) > mart_export_increment_final_ncer_dickel.txt
mv mart_export_increment_final_ncer_dickel.txt ../chromosome_data/mart_export_increment_final_ncer_dickel.txt
cd ..




# # run the actual inference to the applied dataset
cd /group/tran3/duytran/ngocduy.tran/Python\ scripts/
# python inference.py -c mart_export_increment > logging/mart_export_increment.txt
python inference_others.py -c mart_export_increment -f human_encode > logging/mart_export_increment.txt

# 31176664 + 1: chromosome12


#echo ------------------------------------------------------

exit
